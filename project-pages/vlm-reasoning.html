<!-- FILE: project-pages/vlm-reasoning.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <link rel="icon" href="../assets/images/vlm_reasoning_logo.png" />
  <title>Reasoning in Vision-Language Models</title>
  <meta name="description" content="Exploring logical reasoning in VLMs through abductive tasks." />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="../css/layout.css">
  <link rel="stylesheet" href="../css/typography.css">
  <link rel="stylesheet" href="../css/utilities.css">
  <script defer src="../js/script.js"></script>
</head>
<body>
  <div class="navbar">
    <a class="nav-title-link" href="../index.html">
      <span class="nav-title">Shachaf Chaviv</span>
      <a class="button" href="mailto:shachaf.haviv@gmail.com">
        <span class="button-text">Contact Me</span>
      </a>
    </a>
  </div>

  <div id="main-content">
    <div id="project-header">
      <div class="main-title">Reasoning in Vision-Language Models</div>
      <div class="body-text">Evaluated the reasoning ability of VLMs using visual abductive tasks and a custom dataset across styles and prompts.</div>
      <image class="project-header-image" src="../assets/images/vlm_reasoning_logo.png">
    </div>

    <div id="project-details">
      <div class="subheader-text">Project Details / Background</div>
      <div class="project-details-content">
        <div class="body-text">
          This project explored the reasoning ability of modern Vision-Language Models (VLMs)—how well they can interpret images and make logical inferences based on them. I created a dataset of visual abductive reasoning tasks (like "what most likely happened before this image?") and tested models like Florence, MiniCPM, and Idefics.
        </div>
        <div class="body-text">
          To really challenge the models, I generated visuals in different styles: paper cut, anime, sketch, and photo-realistic. I compared responses across models, image styles, and prompt types. The results revealed how surprisingly brittle—or sometimes surprisingly smart—these systems can be. It was part research, part art, and fully fascinating.
        </div>
      </div>
    </div>

    <div id="project-gallery">
      <div class="subheader-text">Image Gallery</div>
      <div class="project-gallery-content">
        <div class="gallery-image-container">
          <img src="../assets/images/vlm_poster.png" class="gallery-image">
          <span class="image-caption">Poster summarizing the VLM reasoning project.</span>
        </div>
        <div class="gallery-image-container half-width">
          <img src="../assets/images/vlm_examples_grid.png" class="gallery-image">
          <span class="image-caption">Example hypotheses and visual representations tested.</span>
        </div>
      </div>
    </div>
  </div>

  <div id="footer">
    <a class="icon-link" href="mailto:shachaf.haviv@gmail.com">
      <image src="../assets/icons/mail.svg" class="footer-icon"/>
    </a>
  </div>
</body>
</html>
